<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>shytab</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-06-19T03:02:29.190Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>shytab</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Blogs</title>
    <link href="http://yoursite.com/2018/06/19/blogs/"/>
    <id>http://yoursite.com/2018/06/19/blogs/</id>
    <published>2018-06-19T02:57:00.000Z</published>
    <updated>2018-06-19T03:02:29.190Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Some-Blogs"><a href="#Some-Blogs" class="headerlink" title="Some Blogs"></a>Some Blogs</h1><h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://shytab.github.io/" alt="Shytab"></h2><h2 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="https://tabsun.github.io/" alt="Tabsun"></h2><h2 id="-2"><a href="#-2" class="headerlink" title=""></a><img src="https://blog.csdn.net/delltdk/" alt="Delltdk"></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Some-Blogs&quot;&gt;&lt;a href=&quot;#Some-Blogs&quot; class=&quot;headerlink&quot; title=&quot;Some Blogs&quot;&gt;&lt;/a&gt;Some Blogs&lt;/h1&gt;&lt;h2 id=&quot;&quot;&gt;&lt;a href=&quot;#&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>FashionAI data explaination</title>
    <link href="http://yoursite.com/2018/03/20/FashionAI-data-explaination/"/>
    <id>http://yoursite.com/2018/03/20/FashionAI-data-explaination/</id>
    <published>2018-03-20T09:27:03.000Z</published>
    <updated>2018-03-20T09:48:02.001Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Keypoints-definition"><a href="#Keypoints-definition" class="headerlink" title="Keypoints definition"></a>Keypoints definition</h1><blockquote><p>A pair of keypoints at symmetric position is put in one clause</p></blockquote><ul><li>neckline_left/neckline_right<ul><li>At the leftmost/rightmost of neckline</li></ul></li><li>center_front<ul><li>At the middle of neck design(including neckline and collar)</li></ul></li><li>shoulder_left/shoulder_right<ul><li>At the cross point of left/right shoulder line and left/right arm line</li></ul></li><li>armpit_left/armpit_right<ul><li>At the cross point of left/right sleeve and front bodice</li></ul></li><li>cuff_left_in/cuff_right_in<ul><li>At the closest point of bottom of left/right cuff to the body.</li></ul></li><li>cuff_left_out/cuff_right_out<ul><li>At the furthest point of bottom of left/right cuff from body.</li></ul></li><li>top_hem_left/top_hem_right<ul><li>At the leftmost/rightmost of blouse or outwear’s hem</li></ul></li><li>waistline_left/waistline_right<ul><li>At the leftmost/rightmost of waist line. Only annotated on clothes with collect waist.</li></ul></li><li>waistband_left/waistband_right<ul><li>At the leftmost/rightmost of waist band.</li></ul></li><li>bottom_left_in/bottom_right_in<ul><li>At the bottom point of left/right trouser leg closest to the body</li></ul></li><li>bottom_left_out/bottom_right_out<ul><li>At the bottom point of left/right trouser leg furthest to the body</li></ul></li><li>hemline_left/hemline_right<ul><li>At the leftmost/rightmost of hemline</li></ul></li><li>crotch<ul><li>At the crotch point of trousers</li></ul></li></ul><div align="center"><br><img src="https://work.alibaba-inc.com/aliwork_tfs/g01_alibaba-inc_com/tfscom/TB1C_AMXACWBuNjy0FaXXXUlXXa.tfsprivate.jpg"><br><br><font color="#0099ff" size="2"> Annotations </font><br></div><h1 id="Keypoints-of-each-Category"><a href="#Keypoints-of-each-Category" class="headerlink" title="Keypoints of each Category"></a>Keypoints of each Category</h1><h2 id="Blouse"><a href="#Blouse" class="headerlink" title="Blouse"></a>Blouse</h2><table><thead><tr><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr><td>neckline_left</td><td>neckline_right</td><td>shoulder_left</td><td>shoulder_right</td><td>center_front</td></tr></tbody></table><table><thead><tr><th>6</th><th>7</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>armpit_left</td><td>armpit_right</td><td>top_hem_left</td><td>top_hem_right</td></tr></tbody></table><table><thead><tr><th>10</th><th>11</th><th>12</th><th>13</th></tr></thead><tbody><tr><td>cuff_left_in</td><td>cuff_left_out</td><td>cuff_right_in</td><td>cuff_right_out</td></tr></tbody></table><h2 id="Outwear"><a href="#Outwear" class="headerlink" title="Outwear"></a>Outwear</h2><table><thead><tr><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr><td>neckline_left</td><td>neckline_right</td><td>shoulder_left</td><td>shoulder_right</td><td>armpit_left</td></tr></tbody></table><table><thead><tr><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>armpit_right</td><td>waistline_left</td><td>waistline_right</td><td>cuff_left_in</td><td>cuff_left_out</td></tr></tbody></table><table><thead><tr><th>11</th><th>12</th><th>13</th><th>14</th></tr></thead><tbody><tr><td>cuff_right_in</td><td>cuff_right_out</td><td>top_hem_left</td><td>top_hem_right</td></tr></tbody></table><h2 id="Trousers"><a href="#Trousers" class="headerlink" title="Trousers"></a>Trousers</h2><table><thead><tr><th>1</th><th>2</th><th>3</th></tr></thead><tbody><tr><td>waistband_left</td><td>waistband_right</td><td>crotch</td></tr></tbody></table><table><thead><tr><th>4</th><th>5</th><th>6</th><th>7</th></tr></thead><tbody><tr><td>bottom_left_in</td><td>bottom_left_out</td><td>bottom_right_in</td><td>bottom_right_out</td></tr></tbody></table><h2 id="Skirt"><a href="#Skirt" class="headerlink" title="Skirt"></a>Skirt</h2><table><thead><tr><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>waistband_left</td><td>waistband_right</td><td>hemline_left</td><td>hemline_right</td></tr></tbody></table><h2 id="Dress"><a href="#Dress" class="headerlink" title="Dress"></a>Dress</h2><table><thead><tr><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody><tr><td>neckline_left</td><td>neckline_right</td><td>shoulder_left</td><td>shoulder_right</td><td>center_front</td></tr></tbody></table><table><thead><tr><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>armpit_left</td><td>armpit_right</td><td>waistline_left</td><td>waistline_right</td><td>cuff_left_in</td></tr></tbody></table><table><thead><tr><th>11</th><th>12</th><th>13</th><th>14</th><th>15</th></tr></thead><tbody><tr><td>cuff_left_out</td><td>cuff_right_in</td><td>cuff_right_out</td><td>hemline_left</td><td>hemline_right</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Keypoints-definition&quot;&gt;&lt;a href=&quot;#Keypoints-definition&quot; class=&quot;headerlink&quot; title=&quot;Keypoints definition&quot;&gt;&lt;/a&gt;Keypoints definition&lt;/h1&gt;&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Playing Doom Game with Deep Reinforcement Learning</title>
    <link href="http://yoursite.com/2018/01/25/Playing-Doom-Game-with-Deep-Reinforcement-Learning/"/>
    <id>http://yoursite.com/2018/01/25/Playing-Doom-Game-with-Deep-Reinforcement-Learning/</id>
    <published>2018-01-25T05:11:38.000Z</published>
    <updated>2018-01-25T06:12:48.977Z</updated>
    
    <content type="html"><![CDATA[<p>FPS（First Person Shooter），第一人称射击游戏，这里特指的是<a href="http://vizdoom.cs.put.edu.pl/" target="_blank" rel="external">Doom</a>游戏。Doom是比较早的FPS游戏，经过修改的VizDoom非常适合做增强学习（Reinforcement Learning）方面的学习和实验。</p><p>下面的视频中是在一个“小房子”里<font color="#ff0000"><strong>AI Bot（左）</strong></font>与一个随意行动的<font color="#00ff00"><strong>Random Bot（右）</strong></font>的对抗过程，在100场Test比赛中战胜Random Bot 95次。可以看到，AI Bot可以做到没有看到敌人时四处寻找，看到敌人时能够快速击毙敌人。而完成这些，模型不需要其他任何输入，仅需要当前游戏的画面，与真实玩家的操作过程没有任何区别。在几次测试中，因为操作不好，我基本没有赢过AI Bot，唯一的可能就是开局赶紧跑到他的背后，否则胜算不大，当然我是比较菜的。</p><div align="center"><br><iframe frameborder="0" width="640" height="560" src="https://v.qq.com/iframe/player.html?vid=a03582ygs8i&tiny=0&auto=0" allowfullscreen></iframe><br></div><br><div align="center"><em>Demo Video: AI bot vs Random bot</em></div><h2 id="1-模型介绍"><a href="#1-模型介绍" class="headerlink" title="1. 模型介绍"></a>1. 模型介绍</h2><p>对于在某个时刻执行某种动作总是可以从环境获得固定响应的游戏，用NEAT [1] 这样的算法可以获得较好的效果，比如玩马里奥游戏时，如果你一直按着前进键不放，每次重生后的游戏过程都会完全一致（掉入某个悬崖或碰上第一个小怪物），而在Doom游戏中如果你一直在原地射击，你的敌人的行动会不同，出场时你的位置也不同，所以几乎不会出现完全一致的游戏过程（敌人可能会从不同角度射杀你或者一直互不相干）。</p><p>同时，与Atari游戏最大的不同是，获取的图像并非游戏的状态State，而是对当前状态的一次观察Observation，即角色周围90°范围内的内容。要解决的问题并不是MDPs(Markov decision processes)问题，而是POMDPs(partially observable MDPs)。DQN [2]在处理action依赖于history states的问题时，不易收敛。实际上在我的多次实验中，稍微复杂的任务都未能收敛。</p><p>在实验中发现，将DQN中的Q值网络分为Policy（下图下半部分计算advantage for actions的部分）与Value（上半部分计算state-value的部分）两个共享卷积层的网络（DDQN的做法[3]），并且将二者在网络末尾再次结合获得action value，这样的网络结构收敛能力得到大大的提高。实际上这种方式的优势是收敛性和泛化能力上都得到了提升[4]。</p><div align="center"><br><img src="https://wx1.sinaimg.cn/mw1024/89ef5361ly1fnssu4krzkj20g005laaf.jpg" alt=""></div><div align="center"><br><em>Net Architecture</em><br></div><h2 id="2-Doom环境"><a href="#2-Doom环境" class="headerlink" title="2. Doom环境"></a>2. Doom环境</h2><p>Project： <a href="https://github.com/Marqt/ViZDoom" target="_blank" rel="external">https://github.com/Marqt/ViZDoom</a></p><h1 id="2-1-Quick-start"><a href="#2-1-Quick-start" class="headerlink" title="2.1 Quick start"></a>2.1 Quick start</h1><p>在另一篇<a href="https://tabsun.github.io/2016/11/25/Lasagne-on-win7-with-GPU-Environment-VizDoom-compilation/" target="_blank" rel="external">Post</a>中，介绍了如何在笔记本上安装CUDA、Theano、Lasagne以及Doom的编译过程。VizDoom在example目录下提供了很多快速上手的示例，对开始使用VizDoom及Lasagne都非常有帮助，建议从learning_theano开始。</p><h1 id="2-2-Interfaces"><a href="#2-2-Interfaces" class="headerlink" title="2.2 Interfaces"></a>2.2 Interfaces</h1><p>下面是Doom提供的几个比较常用的接口：</p><p>make_action: 执行命令并返回reward</p><p>get_state: 获取游戏当前状态</p><p>get_game_variable: 获取游戏中的某些变量，如血量、子弹数量等</p><p>is_episode_finished: 判断当前一局游戏是否已经结束</p><p>new_episode: 新开一局</p><h1 id="2-3-Doom-Builder"><a href="#2-3-Doom-Builder" class="headerlink" title="2.3 Doom Builder"></a>2.3 Doom Builder</h1><p>VizDoom只是提供一个控制Doom游戏的框架，其控制能力是有限的。实际上要打造一个自己的游戏环境，更合适的方式是用<a href="http://www.doombuilder.com/" target="_blank" rel="external">Doom Builder</a>创建自己的地图，可以完成对游戏内容的完全控制。</p><div align="center"><br><img src="https://wx1.sinaimg.cn/mw1024/89ef5361ly1fnssu50qhij20fb0880yu.jpg" alt=""></div><p>它包含Vertices、Linedefs、Sectors、Things、Brightness及Make Sectors共6中mode，常用的是前4种，分别编辑地图中的点、线、区域和事物（比如起始位置、补给箱、弹药等）。这些是用来设计地图中的场景，而执行逻辑要靠<a href="https://zdoom.org/wiki/ACS" target="_blank" rel="external">ACS</a> 脚本。以下面的代码为例，分别定义打开游戏、进入新的episode和角色重生时的动作：角色使用火箭炮，并配备10发炮弹。其他示例可以自行查看scenarios/*.wad文件，所幸这里的代码一般不用很长，所以只要构建出想要的地图和reward机制就可以了（reward机制是重点），Doom中built in的ACS functions可以参见<a href="https://zdoom.org/wiki/Built-in_ACS_functions" target="_blank" rel="external">这里</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#include "zcommon.acs"</span></div><div class="line">script <span class="number">1</span> OPEN</div><div class="line">&#123;</div><div class="line">&#125;</div><div class="line">script <span class="number">2</span> ENTER</div><div class="line">&#123;</div><div class="line">    ClearInventory();</div><div class="line">    GiveInventory(<span class="string">"RocketLauncher"</span>,<span class="number">1</span>);</div><div class="line">    GiveInventory(<span class="string">"RocketAmmo"</span>,<span class="number">10</span>);</div><div class="line">&#125;</div><div class="line">script <span class="number">3</span> RESPAWN</div><div class="line">&#123;</div><div class="line">    ClearInventory();</div><div class="line">    GiveInventory(<span class="string">"RocketLauncher"</span>,<span class="number">1</span>);</div><div class="line">    GiveInventory(<span class="string">"RocketAmmo"</span>,<span class="number">10</span>);</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="3-训练过程"><a href="#3-训练过程" class="headerlink" title="3. 训练过程"></a>3. 训练过程</h2><p>视频中的示例的训练过程总共耗时大概四五天时间（GeForce 940M）。训练过程是非常受制于游戏进程的：如果游戏本身运行速度不够快，训练过程中会有相当一部分时间被耽搁在等待游戏返回reward的过程（make_action）。</p><p>下图展示的是一些常用的优化方法收敛过程的视图：</p><p><div align="center"><br><img src="http://wx2.sinaimg.cn/large/89ef5361ly1fnssu5dgppg20h80dcts9.gif" alt=""><br></div></p><p><div align="center"><br><em>Some opt methods</em><br></div><br>通常训练DQN使用比较多的Rmsprop，不同的优化方法也存在提高训练速度的可能。另一种改进方法是以A3C [5]的方式收集训练transitions，这样游戏进程的速度限制得到缓解。</p><h3 id="4-青出于蓝"><a href="#4-青出于蓝" class="headerlink" title="4. 青出于蓝"></a>4. 青出于蓝</h3><p>在以Random Bot为对手的环境中训练得到AI Bot后，可以尝试以AI Bot为对手，继续训练新的AI Bot2，感兴趣的同学可以试试看青出于蓝是否可以胜于蓝。A被B打败，C又打败B……但是恐怕随着难度的增加，这对训练过程的要求不断提高，是否能够靠同一个模型的不断训练和反馈来得到提高还有待商榷。</p><h2 id="相关阅读："><a href="#相关阅读：" class="headerlink" title="相关阅读："></a>相关阅读：</h2><p>[1]. <a href="http://glenn-roberts.com/posts/tech/2015/07/08/neuroevolution-with-mario.html" target="_blank" rel="external">neuro evolution with mario</a></p><p>[2]. <a href="https://arxiv.org/pdf/1312.5602.pdf" target="_blank" rel="external">Playing Atari with Deep Reinforcement Learning</a></p><p>[3]. <a href="https://arxiv.org/pdf/1509.06461v3.pdf" target="_blank" rel="external">Deep Reinforcement Learning with Double Q-learning</a></p><p>[4]. <a href="https://arxiv.org/pdf/1511.06581v3.pdf" target="_blank" rel="external">Dueling Network Architectures for Deep Reinforcement Learning</a></p><p>[5]. <a href="https://arxiv.org/pdf/1602.01783.pdf" target="_blank" rel="external">Asynchronous Methods for Deep Reinforcement Learning</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;FPS（First Person Shooter），第一人称射击游戏，这里特指的是&lt;a href=&quot;http://vizdoom.cs.put.edu.pl/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Doom&lt;/a&gt;游戏。Doom是比较早的FPS游戏
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>CHALLENGER AI keypoint detection Daily log</title>
    <link href="http://yoursite.com/2017/11/29/CHALLENGER-AI-keypoint-detection-Daily-log/"/>
    <id>http://yoursite.com/2017/11/29/CHALLENGER-AI-keypoint-detection-Daily-log/</id>
    <published>2017-11-29T07:39:36.000Z</published>
    <updated>2017-11-29T08:17:01.087Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Realtime-Multi-Person-Pose-Estimation"><a href="#Realtime-Multi-Person-Pose-Estimation" class="headerlink" title="Realtime Multi-Person Pose Estimation"></a>Realtime Multi-Person Pose Estimation</h2><p><em>0.192</em></p><p>问题一 ： head 和neck两个point在COCO dataset中是不存在的。通过设置关键点之间的推理关系，设定这两个点。</p><p>问题二 ： 12个四肢point虽然看上去检测正确，但OKS很低，说明COCO与CHALLENGE AI的标注有差异，必须重新训练模型。</p><p>SSD并非影响关键，关键是keypoint模型的准确率低。</p><h2 id="Baseline-model：-SSD-DeepLab-2017"><a href="#Baseline-model：-SSD-DeepLab-2017" class="headerlink" title="Baseline model： SSD + DeepLab 2017"></a>Baseline model： SSD + DeepLab 2017</h2><p>(<em>Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</em>)</p><p><em>0.196</em></p><p>问题 ： 训练500k steps, loss 下降至0.9, 是否已经收敛？ 从loss下降过程看，还在收敛中。</p><p>添加random mirror/random scale, 修改input size，以checkpoint继续训练。</p><p><em>0.266</em></p><div align="center"><br><img src="https://wx3.sinaimg.cn/mw1024/89ef5361ly1flz0tqlllhj20m80dmweo.jpg" width="600"><br><br><font color="#0099ff" size="2"> Loss with steps</font><br></div><p>问题 ： loss降低至0.09附近，是否还有下降空间？ 继续训练200k steps未能继续大幅收敛。</p><p>问题 ： 测试集中包含很多rotate sample，尝试0 90 180 270 四个vote决策结果</p><p><em>0.168</em></p><p>rotate+vote不适合dense feature决策过程</p><p>问题 ： train与test的预处理过程有不同，一个是random sample， 一个是直接处理，既然加入了random mirror/mirror scale/pad，应当如何处理test的预处理过程？</p><p>修改image_reader, 严格映射feature map来得到每个point的hot map。</p><p><em>0.33</em></p><p>改换200k steps训练后的结果</p><p><em>0.347</em></p><p>问题 ： 当前只有一个head，就是class，借鉴G_RMI添加offset的head，在checkpoint上fine tune。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Realtime-Multi-Person-Pose-Estimation&quot;&gt;&lt;a href=&quot;#Realtime-Multi-Person-Pose-Estimation&quot; class=&quot;headerlink&quot; title=&quot;Realtime Multi-Per
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>porn detection test results on self-benchmark</title>
    <link href="http://yoursite.com/2017/11/29/porn-detection-test-results-on-self-benchmark-md/"/>
    <id>http://yoursite.com/2017/11/29/porn-detection-test-results-on-self-benchmark-md/</id>
    <published>2017-11-29T07:17:09.000Z</published>
    <updated>2017-12-01T09:57:34.319Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><table><thead><tr><th style="text-align:left">Corp</th><th style="text-align:center">Acc(%)</th></tr></thead><tbody><tr><td style="text-align:left">Baidu Cloud</td><td style="text-align:center">94.3</td></tr><tr><td style="text-align:left">Tupu Tech</td><td style="text-align:center">85.8</td></tr><tr><td style="text-align:left">Youtu</td><td style="text-align:center">86.3</td></tr><tr><td style="text-align:left">Ours ver1.0</td><td style="text-align:center">63.7</td></tr><tr><td style="text-align:left"><strong>Ours ver2.0</strong></td><td style="text-align:center"><strong>86.8</strong></td></tr></tbody></table><p>TO BE CONTINUED<br>Updated @ 2017-11-30</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Results&quot;&gt;&lt;a href=&quot;#Results&quot; class=&quot;headerlink&quot; title=&quot;Results&quot;&gt;&lt;/a&gt;Results&lt;/h1&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;Corp
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>TensorRT + gunicorn + multi-threading，帶你飛起</title>
    <link href="http://yoursite.com/2017/11/10/TensorRT-gunicorn-%E5%A4%9A%E7%B7%9A%E7%A8%8B%EF%BC%8C%E5%B8%B6%E4%BD%A0%E9%A3%9B%E8%B5%B7/"/>
    <id>http://yoursite.com/2017/11/10/TensorRT-gunicorn-多線程，帶你飛起/</id>
    <published>2017-11-10T10:13:59.000Z</published>
    <updated>2017-11-10T11:02:36.823Z</updated>
    
    <content type="html"><![CDATA[<p><em>借助于tensorRT/gunicorn/multithreading，在单个M40机器上，Resnet101能够达到100+pcs/sec的处理速度。在我们的方案中需要下载和上传图像，如果只考虑inference的过程，实际可以达到400pcs/sec的理论速度。</em></p><div align="center"><br><img src="https://wx1.sinaimg.cn/mw1024/89ef5361gy1fld6h59d9bj20cm07vdh1.jpg" width="400" height="200"><br><br><font color="#0099ff" size="2"> each test finish 100 images’ download/upload/process</font><br></div><p><strong>Much Thanks to TensorRT Development Team!</strong></p><p>gunicorn 负责任务的分发，在多个模型instance之间调度任务。</p><p>TensorRT 除了速度快之外，对模型加载后的显存优化也使得同样的GPU计算环境允许加载更多instance(5个resnet27占用显存15% x 12G)，也就有了gunicorn更快的速度。当然实际测试中使用multi-threading后处理时间并非是instance数量的线性关系，而是达到速度上限后即使再多instance也无法再提升。</p><p>理论上来说，这应该是和thread的数量是有关系的。gunicorn+tensorRT也要受到线程最大并发数量的限制，如果没有网络通信对multi-threading的需求，改成本地处理，应当是满足线性的优化速度的。</p><p>PS: <font color="#995500"><strong>threading和multiprocess.pool.ThreadPool都有坑，前者无法唤起线程内的tensorRT调用，后者则很容易使用不当造成线程爆棚</strong></font></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;借助于tensorRT/gunicorn/multithreading，在单个M40机器上，Resnet101能够达到100+pcs/sec的处理速度。在我们的方案中需要下载和上传图像，如果只考虑inference的过程，实际可以达到400pcs/sec的理论速度。
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Batch Normalization为什么好使</title>
    <link href="http://yoursite.com/2017/10/13/2017-10-13-md/"/>
    <id>http://yoursite.com/2017/10/13/2017-10-13-md/</id>
    <published>2017-10-13T05:57:54.000Z</published>
    <updated>2017-10-13T07:58:20.360Z</updated>
    
    <content type="html"><![CDATA[<p><style type="text/css"><br>code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}<br></style></p><script type="text/x-mathjax-config">MathJax.Hub.Config({    tex2jax: {        inlineMath: [['$','$'], ['\\(','\\)']],        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry    }});MathJax.Hub.Queue(function() {    var all = MathJax.Hub.getAllJax(), i;    for(i = 0; i < all.length; i += 1) {        all[i].SourceElement().parentNode.className += ' has-jax';    }});</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><p><a href="https://www.zhihu.com/question/38102762/answer/85238569" target="_blank" rel="external">https://www.zhihu.com/question/38102762/answer/85238569</a></p><ol><li><strong>What</strong> is BN?</li></ol><p>顾名思义，batch normalization嘛，就是“批规范化”咯。Google在ICML文中描述的非常清晰，即在每次SGD时，通过mini-batch来对相应的activation做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1. 而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入（即当$\gamma ^{(k)}= \sqrt{\mathit{Var}\left [ x^{(k)} \right ]}$,$\beta ^{(k)}= E[x^{(k)}]$，从而保证整个network的capacity。（有关capacity的解释：实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大可能会改变某层原来的输入。当然也可能不改变，不改变的时候就是“还原原来输入”。如此一来，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。）</p><p><img src="http://wx4.sinaimg.cn/mw690/89ef5361ly1fkgnl0bdrsj20i70est9q.jpg" alt=""></p><p>关于DNN中的normalization，大家都知道白化（whitening），只是在模型训练过程中进行白化操作会带来过高的计算代价和运算时间。因此本文提出两种简化方式：1）直接对输入信号的每个维度做规范化（“normalize each scalar feature independently”）；2）在每个mini-batch中计算得到mini-batch mean和variance来替代整体训练集的mean和variance. 这便是Algorithm 1.</p><ol><li><strong>How</strong> to Batch Normalize?</li></ol><p>怎样学BN的参数在此就不赘述了，就是经典的chain rule：<br><img src="http://wx1.sinaimg.cn/mw690/89ef5361ly1fkgnkuxsytj20it0agzku.jpg" alt=""></p><ol><li><strong>Where</strong> to use BN?</li></ol><p>BN可以应用于网络中任意的activation set。文中还特别指出在CNN中，BN应作用在非线性映射前，即对$x=Wu+b$做规范化。另外对CNN的“权值共享”策略，BN还有其对应的做法（详见文中3.2节）。</p><ol><li><strong>Why</strong> BN?</li></ol><p>好了，现在才是重头戏－－为什么要用BN？BN work的原因是什么？说到底，BN的提出还是为了克服深度神经网络难以训练的弊病。其实BN背后的insight非常简单，只是在文章中被Google复杂化了。首先来说说“Internal Covariate Shift”。文章的title除了BN这样一个关键词，还有一个便是“ICS”。大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如，transfer learning/domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有$x\in X$，有$P^{s}\left ( Y\mid X= x \right )=P^{t}\left ( Y\mid X=x \right )$,但是$P^{s}(X)\neq P^{t}(X)$. 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。那么好，为什么前面我说Google将其复杂化了。其实如果严格按照解决covariate shift的路子来做的话，大概就是上“importance weight”（ref）之类的机器学习方法。可是这里Google仅仅说“通过mini-batch来规范化某些层/所有层的输入，从而可以固定每层输入信号的均值与方差”就可以解决问题。如果covariate shift可以用这么简单的方法解决，那前人对其的研究也真真是白做了。此外，试想，均值方差一致的分布就是同样的分布吗？当然不是。显然，ICS只是这个问题的“包装纸”嘛，仅仅是一种high-level demonstration。那BN到底是什么原理呢？说到底还是为了<strong>防止“梯度弥散”</strong>。关于梯度弥散，大家都知道一个简单的栗子：$0.9^{30}\approx 0.04$。在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法（见4.2.1节）。</p><ol><li><strong>When</strong> to use BN?</li></ol><p>在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;style type=&quot;text/css&quot;&gt;&lt;br&gt;code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}&lt;br&gt;&lt;/style&gt;&lt;/p&gt;
&lt;script t
      
    
    </summary>
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="Batch Normalization" scheme="http://yoursite.com/tags/Batch-Normalization/"/>
    
  </entry>
  
  <entry>
    <title>贾扬清对caffe中卷积计算的吐槽</title>
    <link href="http://yoursite.com/2017/10/11/%E8%B4%BE%E6%89%AC%E6%B8%85%E5%AF%B9caffe%E4%B8%AD%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E7%9A%84%E5%90%90%E6%A7%BD/"/>
    <id>http://yoursite.com/2017/10/11/贾扬清对caffe中卷积计算的吐槽/</id>
    <published>2017-10-11T13:34:10.000Z</published>
    <updated>2017-10-12T11:19:24.900Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="external">https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo</a></p><p>In the last few months chatting with people about Caffe, a common comment I got was: “<em>Caffe’s convolution has some memory issues.</em>“</p><p>While this is true in some sense, I am not sure whether it is truly an issue - rather, <strong>it is a graduate-student level design choice when I was writing the Caffe framework in just 2 months’ budget with a looming thesis deadline</strong>. It turns out to have its own pros (faster than any trivial implementation unless you optimize really seriously) and cons (large memory consumption). A more detailed explanation follows, if you are interested.</p><p>First of all, convolution is, in some sense, quite hard to optimize. While the conventional definition of convolution in computer vision is usually just a single channel image convolved with a single filter (which is, actually, what Intel IPP’s convolution means), in deep networks we often perform convolution with multiple input channels (the word is usually interchangeable with “depth”) and multiple output channels.</p><p>Loosely speaking, assume that we have a W x H image with depth D at each input location. For each location, we get a K x K patch, which could be considered as a K x K x D vector, and apply M filters to it. In pseudocode, this is (ignoring boundary conditions):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> w <span class="keyword">in</span> 1..W</div><div class="line">  <span class="keyword">for</span> h <span class="keyword">in</span> 1..H</div><div class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> 1..K</div><div class="line">      <span class="keyword">for</span> y <span class="keyword">in</span> 1..K</div><div class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> 1..M</div><div class="line">          <span class="keyword">for</span> d <span class="keyword">in</span> 1..D</div><div class="line">            output(w, h, m) += input(w+x, h+y, d) * filter(m, x, y, d)</div><div class="line">          end</div><div class="line">        end</div><div class="line">      end</div><div class="line">    end</div><div class="line">  end</div><div class="line">end</div></pre></td></tr></table></figure></p><p>Optimizing such a complex nested for loop is non-trivial. If you have tried optimizing matrix multiplication, you know that there is a lot of tricks involved in it. In my third year of PhD I did it in Berkeley’s CS267 (parallel computers) class, and we got the highest maximum performance <a href="http://www.cs.berkeley.edu/~ballard/cs267.sp11/hw1/results/" target="_blank" rel="external">[1]</a> among others after having two layers of caching, unrolled innermost computation, and SSE2 (AVX wasn’t available on the cluster we used). Optimization of convolution could only be more complex.</p><p>I have long admired Alex Krizhevsky’s great optimization in cuda-convnet <a href="http://code.google.com/p/cuda-convnet/" target="_blank" rel="external">[2]</a>. Unfortunately, I had to finish my thesis at the time, and I probably was not as GPU savvy as he is. But I still needed a fast convolution. Thus, I took a simpler approach: reduce the problem to a simpler one, where others have already optimized it really well.</p><p>The trick is to just lay out all the local patches, and organize them to a (W x H, K x K x D) matrix. In Matlab, this is usually known as an im2col operation. After that, consider the filters being a (M, K x K x D) matrix too, the convolution naturally gets reduced to a matrix multiplication (Gemm in BLAS) problem. We have awesome BLAS libraries such as MKL, Atlas, and CuBLAS, with impressive performances. This applies to GPUs as well, although GPU memory is indeed more “precious” than its CPU sibling. However, with reasonably large models such as ImageNet, Caffe has been working pretty OK. It is even able to process videos with such models, thanks to the recent advances in GPU hardware.</p><p>An additional benefit is that, since BLAS is usually optimized for all platforms by the BLAS distributors (like Intel and Nvidia), we don’t need to worry about optimization with specific platforms, and can sit back comfortably assuming that a reasonably fast speed can be achieved on almost all platforms that those BLAS libraries support.</p><p>Somewhat surprising to me is that, such a “lazy optimization” is quite efficient, better than several other approaches and only recently been beaten (as expected) by the mighty Krizhevsky’s optimized cuda-convnet2 code <a href="https://code.google.com/p/cuda-convnet2/" target="_blank" rel="external">[3]</a>:</p><p><a href="https://github.com/soumith/convnet-benchmarks" target="_blank" rel="external">https://github.com/soumith/convnet-benchmarks</a> (as of Jul 27, 2014)</p><p>So this is the story of the memory issue that came into play. I didn’t mean to defend it - in any means, it was designed as a <strong>temporary</strong> solution. Whenever this word pops up, it reminds me of this <a href="http://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered" target="_blank" rel="external">[4]</a>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// somedev1 -  6/7/02 Adding temporary tracking of Login screen</div><div class="line">// somedev2 -  5/22/07 Temporary my ass</div></pre></td></tr></table></figure></p><p>So we are having a lot of improvements being worked on by Berkeley folks and collaborators - things will apparently get better soon - expect a faster yet more memory-efficient convolution this fall.</p><p>For me, the lesson learned when writing Caffe as a grad student is simplicity: <strong>reduce my problem to one already solved</strong>. It sometimes burns <a href="http://www.reddit.com/r/Jokes/comments/27bgl8/a_math_joke/" target="_blank" rel="external">[5]</a>, but could always be improved later if necessary.</p><p>[1] <a href="http://www.cs.berkeley.edu/~ballard/cs267.sp11/hw1/results/" target="_blank" rel="external">http://www.cs.berkeley.edu/~ballard/cs267.sp11/hw1/results/</a></p><p>[2] <a href="http://code.google.com/p/cuda-convnet/" target="_blank" rel="external">http://code.google.com/p/cuda-convnet/</a></p><p>[3] <a href="https://code.google.com/p/cuda-convnet2/" target="_blank" rel="external">https://code.google.com/p/cuda-convnet2/</a></p><p>[4] <a href="http://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered" target="_blank" rel="external">http://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered</a></p><p>[5] <a href="http://www.reddit.com/r/Jokes/comments/27bgl8/a_math_joke/" target="_blank" rel="external">http://www.reddit.com/r/Jokes/comments/27bgl8/a_math_joke/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/Yangqing/
      
    
    </summary>
    
      <category term="开发" scheme="http://yoursite.com/categories/%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="转载" scheme="http://yoursite.com/tags/%E8%BD%AC%E8%BD%BD/"/>
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
      <category term="开发" scheme="http://yoursite.com/tags/%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/10/10/hello-world/"/>
    <id>http://yoursite.com/2017/10/10/hello-world/</id>
    <published>2017-10-10T13:36:45.000Z</published>
    <updated>2017-10-12T11:20:54.366Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
      <category term="cate" scheme="http://yoursite.com/categories/cate/"/>
    
    
      <category term="live" scheme="http://yoursite.com/tags/live/"/>
    
      <category term="new" scheme="http://yoursite.com/tags/new/"/>
    
  </entry>
  
</feed>
