<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>shytab</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-29T08:17:01.087Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>shytab</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CHALLENGER AI keypoint detection Daily log</title>
    <link href="http://yoursite.com/2017/11/29/CHALLENGER-AI-keypoint-detection-Daily-log/"/>
    <id>http://yoursite.com/2017/11/29/CHALLENGER-AI-keypoint-detection-Daily-log/</id>
    <published>2017-11-29T07:39:36.000Z</published>
    <updated>2017-11-29T08:17:01.087Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Realtime-Multi-Person-Pose-Estimation"><a href="#Realtime-Multi-Person-Pose-Estimation" class="headerlink" title="Realtime Multi-Person Pose Estimation"></a>Realtime Multi-Person Pose Estimation</h2><p><em>0.192</em></p><p>问题一 ： head 和neck两个point在COCO dataset中是不存在的。通过设置关键点之间的推理关系，设定这两个点。</p><p>问题二 ： 12个四肢point虽然看上去检测正确，但OKS很低，说明COCO与CHALLENGE AI的标注有差异，必须重新训练模型。</p><p>SSD并非影响关键，关键是keypoint模型的准确率低。</p><h2 id="Baseline-model：-SSD-DeepLab-2017"><a href="#Baseline-model：-SSD-DeepLab-2017" class="headerlink" title="Baseline model： SSD + DeepLab 2017"></a>Baseline model： SSD + DeepLab 2017</h2><p>(<em>Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</em>)</p><p><em>0.196</em></p><p>问题 ： 训练500k steps, loss 下降至0.9, 是否已经收敛？ 从loss下降过程看，还在收敛中。</p><p>添加random mirror/random scale, 修改input size，以checkpoint继续训练。</p><p><em>0.266</em></p><div align="center"><br><img src="https://wx3.sinaimg.cn/mw1024/89ef5361ly1flz0tqlllhj20m80dmweo.jpg" width="600"><br><br><font color="#0099ff" size="2"> Loss with steps</font><br></div><p>问题 ： loss降低至0.09附近，是否还有下降空间？ 继续训练200k steps未能继续大幅收敛。</p><p>问题 ： 测试集中包含很多rotate sample，尝试0 90 180 270 四个vote决策结果</p><p><em>0.168</em></p><p>rotate+vote不适合dense feature决策过程</p><p>问题 ： train与test的预处理过程有不同，一个是random sample， 一个是直接处理，既然加入了random mirror/mirror scale/pad，应当如何处理test的预处理过程？</p><p>修改image_reader, 严格映射feature map来得到每个point的hot map。</p><p><em>0.33</em></p><p>改换200k steps训练后的结果</p><p><em>0.347</em></p><p>问题 ： 当前只有一个head，就是class，借鉴G_RMI添加offset的head，在checkpoint上fine tune。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Realtime-Multi-Person-Pose-Estimation&quot;&gt;&lt;a href=&quot;#Realtime-Multi-Person-Pose-Estimation&quot; class=&quot;headerlink&quot; title=&quot;Realtime Multi-Per
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>porn detection test results on self-benchmark</title>
    <link href="http://yoursite.com/2017/11/29/porn-detection-test-results-on-self-benchmark-md/"/>
    <id>http://yoursite.com/2017/11/29/porn-detection-test-results-on-self-benchmark-md/</id>
    <published>2017-11-29T07:17:09.000Z</published>
    <updated>2017-11-29T07:31:30.685Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><table><thead><tr><th style="text-align:left">Corp</th><th style="text-align:center">Acc(%)</th></tr></thead><tbody><tr><td style="text-align:left">Baidu Cloud</td><td style="text-align:center">86.7%</td></tr><tr><td style="text-align:left">Tupu Tech</td><td style="text-align:center">77.8%</td></tr><tr><td style="text-align:left"><strong>Ours</strong></td><td style="text-align:center"><strong>83.8%</strong></td></tr></tbody></table><p>TO BE CONTINUED</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Results&quot;&gt;&lt;a href=&quot;#Results&quot; class=&quot;headerlink&quot; title=&quot;Results&quot;&gt;&lt;/a&gt;Results&lt;/h1&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;Corp
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>TensorRT + gunicorn + multi-threading，帶你飛起</title>
    <link href="http://yoursite.com/2017/11/10/TensorRT-gunicorn-%E5%A4%9A%E7%B7%9A%E7%A8%8B%EF%BC%8C%E5%B8%B6%E4%BD%A0%E9%A3%9B%E8%B5%B7/"/>
    <id>http://yoursite.com/2017/11/10/TensorRT-gunicorn-多線程，帶你飛起/</id>
    <published>2017-11-10T10:13:59.000Z</published>
    <updated>2017-11-10T11:02:36.823Z</updated>
    
    <content type="html"><![CDATA[<p><em>借助于tensorRT/gunicorn/multithreading，在单个M40机器上，Resnet101能够达到100+pcs/sec的处理速度。在我们的方案中需要下载和上传图像，如果只考虑inference的过程，实际可以达到400pcs/sec的理论速度。</em></p><div align="center"><br><img src="https://wx1.sinaimg.cn/mw1024/89ef5361gy1fld6h59d9bj20cm07vdh1.jpg" width="400" height="200"><br><br><font color="#0099ff" size="2"> each test finish 100 images’ download/upload/process</font><br></div><p><strong>Much Thanks to TensorRT Development Team!</strong></p><p>gunicorn 负责任务的分发，在多个模型instance之间调度任务。</p><p>TensorRT 除了速度快之外，对模型加载后的显存优化也使得同样的GPU计算环境允许加载更多instance(5个resnet27占用显存15% x 12G)，也就有了gunicorn更快的速度。当然实际测试中使用multi-threading后处理时间并非是instance数量的线性关系，而是达到速度上限后即使再多instance也无法再提升。</p><p>理论上来说，这应该是和thread的数量是有关系的。gunicorn+tensorRT也要受到线程最大并发数量的限制，如果没有网络通信对multi-threading的需求，改成本地处理，应当是满足线性的优化速度的。</p><p>PS: <font color="#995500"><strong>threading和multiprocess.pool.ThreadPool都有坑，前者无法唤起线程内的tensorRT调用，后者则很容易使用不当造成线程爆棚</strong></font></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;借助于tensorRT/gunicorn/multithreading，在单个M40机器上，Resnet101能够达到100+pcs/sec的处理速度。在我们的方案中需要下载和上传图像，如果只考虑inference的过程，实际可以达到400pcs/sec的理论速度。
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Batch Normalization为什么好使</title>
    <link href="http://yoursite.com/2017/10/13/2017-10-13-md/"/>
    <id>http://yoursite.com/2017/10/13/2017-10-13-md/</id>
    <published>2017-10-13T05:57:54.000Z</published>
    <updated>2017-10-13T07:58:20.360Z</updated>
    
    <content type="html"><![CDATA[<p><style type="text/css"><br>code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}<br></style></p><script type="text/x-mathjax-config">MathJax.Hub.Config({    tex2jax: {        inlineMath: [['$','$'], ['\\(','\\)']],        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry    }});MathJax.Hub.Queue(function() {    var all = MathJax.Hub.getAllJax(), i;    for(i = 0; i < all.length; i += 1) {        all[i].SourceElement().parentNode.className += ' has-jax';    }});</script><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><p><a href="https://www.zhihu.com/question/38102762/answer/85238569" target="_blank" rel="external">https://www.zhihu.com/question/38102762/answer/85238569</a></p><ol><li><strong>What</strong> is BN?</li></ol><p>顾名思义，batch normalization嘛，就是“批规范化”咯。Google在ICML文中描述的非常清晰，即在每次SGD时，通过mini-batch来对相应的activation做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1. 而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入（即当$\gamma ^{(k)}= \sqrt{\mathit{Var}\left [ x^{(k)} \right ]}$,$\beta ^{(k)}= E[x^{(k)}]$，从而保证整个network的capacity。（有关capacity的解释：实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大可能会改变某层原来的输入。当然也可能不改变，不改变的时候就是“还原原来输入”。如此一来，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。）</p><p><img src="http://wx4.sinaimg.cn/mw690/89ef5361ly1fkgnl0bdrsj20i70est9q.jpg" alt=""></p><p>关于DNN中的normalization，大家都知道白化（whitening），只是在模型训练过程中进行白化操作会带来过高的计算代价和运算时间。因此本文提出两种简化方式：1）直接对输入信号的每个维度做规范化（“normalize each scalar feature independently”）；2）在每个mini-batch中计算得到mini-batch mean和variance来替代整体训练集的mean和variance. 这便是Algorithm 1.</p><ol><li><strong>How</strong> to Batch Normalize?</li></ol><p>怎样学BN的参数在此就不赘述了，就是经典的chain rule：<br><img src="http://wx1.sinaimg.cn/mw690/89ef5361ly1fkgnkuxsytj20it0agzku.jpg" alt=""></p><ol><li><strong>Where</strong> to use BN?</li></ol><p>BN可以应用于网络中任意的activation set。文中还特别指出在CNN中，BN应作用在非线性映射前，即对$x=Wu+b$做规范化。另外对CNN的“权值共享”策略，BN还有其对应的做法（详见文中3.2节）。</p><ol><li><strong>Why</strong> BN?</li></ol><p>好了，现在才是重头戏－－为什么要用BN？BN work的原因是什么？说到底，BN的提出还是为了克服深度神经网络难以训练的弊病。其实BN背后的insight非常简单，只是在文章中被Google复杂化了。首先来说说“Internal Covariate Shift”。文章的title除了BN这样一个关键词，还有一个便是“ICS”。大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如，transfer learning/domain adaptation等。而covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有$x\in X$，有$P^{s}\left ( Y\mid X= x \right )=P^{t}\left ( Y\mid X=x \right )$,但是$P^{s}(X)\neq P^{t}(X)$. 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。那么好，为什么前面我说Google将其复杂化了。其实如果严格按照解决covariate shift的路子来做的话，大概就是上“importance weight”（ref）之类的机器学习方法。可是这里Google仅仅说“通过mini-batch来规范化某些层/所有层的输入，从而可以固定每层输入信号的均值与方差”就可以解决问题。如果covariate shift可以用这么简单的方法解决，那前人对其的研究也真真是白做了。此外，试想，均值方差一致的分布就是同样的分布吗？当然不是。显然，ICS只是这个问题的“包装纸”嘛，仅仅是一种high-level demonstration。那BN到底是什么原理呢？说到底还是为了<strong>防止“梯度弥散”</strong>。关于梯度弥散，大家都知道一个简单的栗子：$0.9^{30}\approx 0.04$。在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法（见4.2.1节）。</p><ol><li><strong>When</strong> to use BN?</li></ol><p>在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;style type=&quot;text/css&quot;&gt;&lt;br&gt;code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}&lt;br&gt;&lt;/style&gt;&lt;/p&gt;
&lt;script t
      
    
    </summary>
    
      <category term="算法" scheme="http://yoursite.com/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="Batch Normalization" scheme="http://yoursite.com/tags/Batch-Normalization/"/>
    
  </entry>
  
  <entry>
    <title>贾扬清对caffe中卷积计算的吐槽</title>
    <link href="http://yoursite.com/2017/10/11/%E8%B4%BE%E6%89%AC%E6%B8%85%E5%AF%B9caffe%E4%B8%AD%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E7%9A%84%E5%90%90%E6%A7%BD/"/>
    <id>http://yoursite.com/2017/10/11/贾扬清对caffe中卷积计算的吐槽/</id>
    <published>2017-10-11T13:34:10.000Z</published>
    <updated>2017-10-12T11:19:24.900Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="external">https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo</a></p><p>In the last few months chatting with people about Caffe, a common comment I got was: “<em>Caffe’s convolution has some memory issues.</em>“</p><p>While this is true in some sense, I am not sure whether it is truly an issue - rather, <strong>it is a graduate-student level design choice when I was writing the Caffe framework in just 2 months’ budget with a looming thesis deadline</strong>. It turns out to have its own pros (faster than any trivial implementation unless you optimize really seriously) and cons (large memory consumption). A more detailed explanation follows, if you are interested.</p><p>First of all, convolution is, in some sense, quite hard to optimize. While the conventional definition of convolution in computer vision is usually just a single channel image convolved with a single filter (which is, actually, what Intel IPP’s convolution means), in deep networks we often perform convolution with multiple input channels (the word is usually interchangeable with “depth”) and multiple output channels.</p><p>Loosely speaking, assume that we have a W x H image with depth D at each input location. For each location, we get a K x K patch, which could be considered as a K x K x D vector, and apply M filters to it. In pseudocode, this is (ignoring boundary conditions):<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> w <span class="keyword">in</span> 1..W</div><div class="line">  <span class="keyword">for</span> h <span class="keyword">in</span> 1..H</div><div class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> 1..K</div><div class="line">      <span class="keyword">for</span> y <span class="keyword">in</span> 1..K</div><div class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> 1..M</div><div class="line">          <span class="keyword">for</span> d <span class="keyword">in</span> 1..D</div><div class="line">            output(w, h, m) += input(w+x, h+y, d) * filter(m, x, y, d)</div><div class="line">          end</div><div class="line">        end</div><div class="line">      end</div><div class="line">    end</div><div class="line">  end</div><div class="line">end</div></pre></td></tr></table></figure></p><p>Optimizing such a complex nested for loop is non-trivial. If you have tried optimizing matrix multiplication, you know that there is a lot of tricks involved in it. In my third year of PhD I did it in Berkeley’s CS267 (parallel computers) class, and we got the highest maximum performance <a href="http://www.cs.berkeley.edu/~ballard/cs267.sp11/hw1/results/" target="_blank" rel="external">[1]</a> among others after having two layers of caching, unrolled innermost computation, and SSE2 (AVX wasn’t available on the cluster we used). Optimization of convolution could only be more complex.</p><p>I have long admired Alex Krizhevsky’s great optimization in cuda-convnet <a href="http://code.google.com/p/cuda-convnet/" target="_blank" rel="external">[2]</a>. Unfortunately, I had to finish my thesis at the time, and I probably was not as GPU savvy as he is. But I still needed a fast convolution. Thus, I took a simpler approach: reduce the problem to a simpler one, where others have already optimized it really well.</p><p>The trick is to just lay out all the local patches, and organize them to a (W x H, K x K x D) matrix. In Matlab, this is usually known as an im2col operation. After that, consider the filters being a (M, K x K x D) matrix too, the convolution naturally gets reduced to a matrix multiplication (Gemm in BLAS) problem. We have awesome BLAS libraries such as MKL, Atlas, and CuBLAS, with impressive performances. This applies to GPUs as well, although GPU memory is indeed more “precious” than its CPU sibling. However, with reasonably large models such as ImageNet, Caffe has been working pretty OK. It is even able to process videos with such models, thanks to the recent advances in GPU hardware.</p><p>An additional benefit is that, since BLAS is usually optimized for all platforms by the BLAS distributors (like Intel and Nvidia), we don’t need to worry about optimization with specific platforms, and can sit back comfortably assuming that a reasonably fast speed can be achieved on almost all platforms that those BLAS libraries support.</p><p>Somewhat surprising to me is that, such a “lazy optimization” is quite efficient, better than several other approaches and only recently been beaten (as expected) by the mighty Krizhevsky’s optimized cuda-convnet2 code <a href="https://code.google.com/p/cuda-convnet2/" target="_blank" rel="external">[3]</a>:</p><p><a href="https://github.com/soumith/convnet-benchmarks" target="_blank" rel="external">https://github.com/soumith/convnet-benchmarks</a> (as of Jul 27, 2014)</p><p>So this is the story of the memory issue that came into play. I didn’t mean to defend it - in any means, it was designed as a <strong>temporary</strong> solution. Whenever this word pops up, it reminds me of this <a href="http://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered" target="_blank" rel="external">[4]</a>:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// somedev1 -  6/7/02 Adding temporary tracking of Login screen</div><div class="line">// somedev2 -  5/22/07 Temporary my ass</div></pre></td></tr></table></figure></p><p>So we are having a lot of improvements being worked on by Berkeley folks and collaborators - things will apparently get better soon - expect a faster yet more memory-efficient convolution this fall.</p><p>For me, the lesson learned when writing Caffe as a grad student is simplicity: <strong>reduce my problem to one already solved</strong>. It sometimes burns <a href="http://www.reddit.com/r/Jokes/comments/27bgl8/a_math_joke/" target="_blank" rel="external">[5]</a>, but could always be improved later if necessary.</p><p>[1] <a href="http://www.cs.berkeley.edu/~ballard/cs267.sp11/hw1/results/" target="_blank" rel="external">http://www.cs.berkeley.edu/~ballard/cs267.sp11/hw1/results/</a></p><p>[2] <a href="http://code.google.com/p/cuda-convnet/" target="_blank" rel="external">http://code.google.com/p/cuda-convnet/</a></p><p>[3] <a href="https://code.google.com/p/cuda-convnet2/" target="_blank" rel="external">https://code.google.com/p/cuda-convnet2/</a></p><p>[4] <a href="http://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered" target="_blank" rel="external">http://stackoverflow.com/questions/184618/what-is-the-best-comment-in-source-code-you-have-ever-encountered</a></p><p>[5] <a href="http://www.reddit.com/r/Jokes/comments/27bgl8/a_math_joke/" target="_blank" rel="external">http://www.reddit.com/r/Jokes/comments/27bgl8/a_math_joke/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;https://github.com/Yangqing/
      
    
    </summary>
    
      <category term="开发" scheme="http://yoursite.com/categories/%E5%BC%80%E5%8F%91/"/>
    
    
      <category term="转载" scheme="http://yoursite.com/tags/%E8%BD%AC%E8%BD%BD/"/>
    
      <category term="caffe" scheme="http://yoursite.com/tags/caffe/"/>
    
      <category term="开发" scheme="http://yoursite.com/tags/%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/10/10/hello-world/"/>
    <id>http://yoursite.com/2017/10/10/hello-world/</id>
    <published>2017-10-10T13:36:45.000Z</published>
    <updated>2017-10-12T11:20:54.366Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
      <category term="cate" scheme="http://yoursite.com/categories/cate/"/>
    
    
      <category term="live" scheme="http://yoursite.com/tags/live/"/>
    
      <category term="new" scheme="http://yoursite.com/tags/new/"/>
    
  </entry>
  
</feed>
